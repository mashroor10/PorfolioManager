{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34941902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from sqlalchemy import create_engine, inspect, text\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import abstractmethod\n",
    "from itertools import combinations\n",
    "from dataclasses import dataclass\n",
    "from backtesting import Backtest, Strategy\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "import seaborn as sns\n",
    "import math\n",
    "import time\n",
    "import concurrent.futures\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"api_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81536b16",
   "metadata": {},
   "source": [
    "##### Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc83259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostgresManager:\n",
    "    def __init__(self, host, port, dbname, user, password):\n",
    "        self.db_params = {\n",
    "            'host': host,\n",
    "            'port': port,\n",
    "            'dbname': dbname,\n",
    "            'user': user,\n",
    "            'password': password\n",
    "        }\n",
    "        self.engine_str = (\n",
    "            f\"postgresql://{user}:{password}@{host}:{port}/{dbname}\"\n",
    "        )\n",
    "        self.engine = create_engine(self.engine_str)\n",
    "        print(\"✅ PostgreSQL connection initialized.\")\n",
    "\n",
    "    def upload_dataframe(self, df: pd.DataFrame, table_name: str, if_exists='replace'):\n",
    "        \"\"\"\n",
    "        Uploads a DataFrame to PostgreSQL.\n",
    "        - if_exists: 'replace', 'append', or 'fail'\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df.to_sql(table_name, self.engine, if_exists=if_exists, index=False, method='multi')\n",
    "            print(f\"✅ Data uploaded to table '{table_name}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to upload to '{table_name}': {e}\")\n",
    "    \n",
    "    def getTicker30MinData(self, ticker: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Retrieves 30-minute interval data for a given ticker from PostgreSQL\n",
    "        \"\"\"\n",
    "        table_name = f\"{ticker.upper()}_30MinData\"\n",
    "        try:\n",
    "            query = f\"SELECT * FROM \\\"{table_name}\\\"\"\n",
    "            df = pd.read_sql_query(query, self.engine)\n",
    "            \n",
    "            # Convert 'date' column to datetime and set as index\n",
    "            if 'date' in df.columns:\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "                df.set_index('date', inplace=True)\n",
    "                df.sort_index(inplace=True)\n",
    "                print(f\"✅ Retrieved {len(df)} rows of 30-min data for {ticker}\")\n",
    "            else:\n",
    "                print(f\"❌ 'date' column missing in table {table_name}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to fetch data for {ticker}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    def getTickerEODData(self, ticker: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Retrieves EOD data for a given ticker from PostgreSQL\n",
    "        \"\"\"\n",
    "        table_name = f\"{ticker.upper()}_EOD_Data\"\n",
    "        try:\n",
    "            query = f\"SELECT * FROM \\\"{table_name}\\\"\"\n",
    "            df = pd.read_sql_query(query, self.engine)\n",
    "            \n",
    "            # Convert 'date' column to datetime and set as index\n",
    "            if 'date' in df.columns:\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "                df.set_index('date', inplace=True)\n",
    "                df.sort_index(inplace=True)\n",
    "                print(f\"✅ Retrieved {len(df)} rows of EOD data for {ticker}\")\n",
    "            else:\n",
    "                print(f\"❌ 'date' column missing in table {table_name}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to fetch data for {ticker}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def getTickerFundamentalsData(self, ticker: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Retrieves fundamentals data for a given ticker from PostgreSQL\n",
    "        \"\"\"\n",
    "        table_name = f\"{ticker.upper()}_FundamentalsData\"\n",
    "        try:\n",
    "            query = f\"SELECT * FROM \\\"{table_name}\\\"\"\n",
    "            df = pd.read_sql_query(query, self.engine)\n",
    "            \n",
    "            # Convert 'date' column to datetime and set as index\n",
    "            if 'date' in df.columns:\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "                df.set_index('date', inplace=True)\n",
    "                df.sort_index(inplace=True)\n",
    "                print(f\"✅ Retrieved {len(df)} rows of fundamentals data for {ticker}\")\n",
    "            else:\n",
    "                print(f\"❌ 'date' column missing in table {table_name}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to fetch data for {ticker}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    def getTickerCombinedData(self, ticker: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Retrieves combined data for a given ticker from PostgreSQL\n",
    "        \"\"\"\n",
    "        table_name = f\"{ticker.upper()}_CombinedData\"\n",
    "        try:\n",
    "            query = f\"SELECT * FROM \\\"{table_name}\\\"\"\n",
    "            df = pd.read_sql_query(query, self.engine)\n",
    "            \n",
    "            # Convert 'date' column to datetime and set as index\n",
    "            if 'date' in df.columns:\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "                df.set_index('date', inplace=True)\n",
    "                df.sort_index(inplace=True)\n",
    "                print(f\"✅ Retrieved {len(df)} rows of combined data for {ticker}\")\n",
    "            else:\n",
    "                print(f\"❌ 'date' column missing in table {table_name}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to fetch data for {ticker}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "    def get_tickers_from_30min_tables(self):\n",
    "        \"\"\"\n",
    "        Extracts all tickers from tables that match the {ticker}_30MinData format.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            inspector = inspect(self.engine)\n",
    "            all_tables = inspector.get_table_names()\n",
    "            pattern = re.compile(r'^(.*)_30MinData$', re.IGNORECASE)\n",
    "            tickers = [match.group(1).upper() for table in all_tables if (match := pattern.match(table))]\n",
    "            return tickers\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to inspect tables: {e}\")\n",
    "            return []\n",
    "    def get_tickers_from_EOD_tables(self):\n",
    "        \"\"\"\n",
    "        Extracts all tickers from tables that match the {ticker}_30MinData format.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            inspector = inspect(self.engine)\n",
    "            all_tables = inspector.get_table_names()\n",
    "            pattern = re.compile(r'^(.*)_EOD_Data$', re.IGNORECASE)\n",
    "            tickers = [match.group(1).upper() for table in all_tables if (match := pattern.match(table))]\n",
    "            return tickers\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to inspect tables: {e}\")\n",
    "            return []\n",
    "        \n",
    "    def get_tickers_from_Fundamentals_tables(self):\n",
    "        \"\"\"\n",
    "        Extracts all tickers from tables that match the {ticker}_Fundamentals format.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            inspector = inspect(self.engine)\n",
    "            all_tables = inspector.get_table_names()\n",
    "            pattern = re.compile(r'^(.*)_FundamentalsData$', re.IGNORECASE)\n",
    "            tickers = [match.group(1).upper() for table in all_tables if (match := pattern.match(table))]\n",
    "            return tickers\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to inspect tables: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_tickers_from_combined_tables(self):\n",
    "        try:\n",
    "            inspector = inspect(self.engine)\n",
    "            all_tables = inspector.get_table_names()\n",
    "            pattern = re.compile(r'^(.*)_CombinedData$', re.IGNORECASE)\n",
    "            tickers = [match.group(1).upper() for table in all_tables if (match := pattern.match(table))]\n",
    "            return tickers\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to inspect tables: {e}\")\n",
    "            return []\n",
    "        \n",
    "#example usage\n",
    "pg = PostgresManager(\n",
    "    host=os.getenv('host'),\n",
    "    port=os.getenv('port'),\n",
    "    dbname=os.getenv('dbname'),\n",
    "    user=os.getenv('user'),\n",
    "    password=os.getenv('password')\n",
    ")\n",
    "# Get all tickers from 30-min tables\n",
    "tickers = pg.get_tickers_from_30min_tables()\n",
    "print(\"Tickers found:\", tickers)\n",
    "\n",
    "# Fetch 30-min data for a specific ticker\n",
    "ticker_data = pg.getTicker30MinData('AAPL')\n",
    "print(ticker_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565ef5d6",
   "metadata": {},
   "source": [
    "##### Getting EOD Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f6962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get valid symbols from CompanyProfiles\n",
    "query = \"\"\"\n",
    "SELECT symbol FROM \"CompanyProfiles\"\n",
    "WHERE \"marketCap\" IS NOT NULL AND \"marketCap\" > 0\n",
    "\"\"\"\n",
    "symbols_df = pd.read_sql_query(text(query), pg.engine)\n",
    "tickers = symbols_df['symbol'].tolist()\n",
    "\n",
    "print(f\"✅ Retrieved {len(tickers)} tickers with market cap > 0\")\n",
    "\n",
    "# 2. Get valid EODData tables that exist in the database\n",
    "existing_EOD_tables = pg.get_tickers_from_EOD_tables()\n",
    "valid_EOD_tickers = [t for t in tickers if t in existing_EOD_tables]\n",
    "existing_Fundamentals_tables = pg.get_tickers_from_Fundamentals_tables()\n",
    "valid_tickers = [t for t in valid_EOD_tickers if t in existing_Fundamentals_tables]\n",
    "print(f\"✅ Found {len(existing_EOD_tables)} existing EODData tables\")\n",
    "print(f\"✅ Found {len(existing_Fundamentals_tables)} existing FundamentalsData tables\")\n",
    "print(f\"✅ {len(valid_EOD_tickers)} tickers with available EODData\")\n",
    "print(f\"✅ {len(existing_Fundamentals_tables)} tickers with available FundamentalsData\")\n",
    "print(f\"✅ {len(valid_tickers)} tickers with available FundamentalsData\")\n",
    "\n",
    "print(f\"✅ {len(valid_tickers)} tickers with available EODData\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674da6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnsOfInterest = ['filingDate_income', 'ebitda', 'ebit', 'grossProfit', 'revenue', 'researchAndDevelopmentExpenses',\n",
    "                     'costAndExpenses', 'totalCurrentLiabilities', 'weightedAverageShsOut', 'epsDiluted', 'totalLiabilities','totalAssets']\n",
    "\n",
    "combinedDataDict = {}\n",
    "existing_tables = pg.get_tickers_from_combined_tables()\n",
    "\n",
    "for ticker in valid_tickers:\n",
    "    try:\n",
    "        dataframeName = f\"{ticker}_CombinedData\"\n",
    "        # Check if the combined data already exists\n",
    "        \n",
    "        fundamentals_df = pg.getTickerFundamentalsData(ticker)\n",
    "        if fundamentals_df.empty:\n",
    "            print(f\"❌ No fundamentals data for {ticker}\")\n",
    "            continue\n",
    "        \n",
    "        # Filter columns of interest\n",
    "        filtered_df = fundamentals_df[columnsOfInterest]\n",
    "        \n",
    "        # Convert 'filingDate_income' to datetime\n",
    "        filtered_df['filingDate_income'] = pd.to_datetime(filtered_df['filingDate_income'], errors='coerce')\n",
    "        \n",
    "        # Drop rows with NaN values in 'filingDate_income'\n",
    "        filtered_df.dropna(subset=['filingDate_income'], inplace=True)\n",
    "        \n",
    "        # Set 'filingDate_income' as index\n",
    "        #filtered_df.set_index('filingDate_income', inplace=True)\n",
    "        \n",
    "        #getting the EOD data for the same ticker\n",
    "        eod_df = pg.getTickerEODData(ticker)\n",
    "        if eod_df.empty:\n",
    "            print(f\"❌ No EOD data for {ticker}\")\n",
    "            continue\n",
    "        # Merge EOD data with fundamentals on date index\n",
    "        # forward fill missing values when combining the two dataframes\n",
    "        combined_df = eod_df.join(filtered_df, how='outer', lsuffix='_eod', rsuffix='_fundamentals')\n",
    "        combined_df.ffill(inplace=True)\n",
    "        combined_df.dropna(inplace=True)  # Drop rows with any NaN values after merging\n",
    "        if combined_df.empty:\n",
    "            print(f\"❌ Combined data is empty for {ticker}\")\n",
    "            continue\n",
    "        #the market cap is the 'weightedAverageShsOut' * 'close'\n",
    "        combined_df['marketCap'] = combined_df['weightedAverageShsOut'] * combined_df['close']\n",
    "        combined_df['bookValue'] = combined_df['totalAssets'] - combined_df['totalLiabilities']\n",
    "        combined_df['debtToEquity'] = combined_df['totalLiabilities'] / ((combined_df['totalAssets'] - combined_df['totalLiabilities']))\n",
    "        combined_df['bookToMarket'] = combined_df['bookValue'] / combined_df['marketCap']\n",
    "        combined_df['enterpriseValue'] = combined_df['marketCap'] + combined_df['totalLiabilities'] - combined_df['totalAssets']\n",
    "        combined_df['ebitValue'] = combined_df['ebitda'] / combined_df['enterpriseValue']\n",
    "        combined_df['ebitdaValue'] = combined_df['ebitda'] / combined_df['marketCap']\n",
    "        combined_df['profitValue'] = combined_df['grossProfit'] / combined_df['marketCap']\n",
    "        combinedDataDict[ticker] = combined_df.copy()\n",
    "        combined_df.reset_index(inplace=True)  # Reset index to have 'date' as a column\n",
    "        \n",
    "        pg.upload_dataframe(combined_df, dataframeName, if_exists='replace')\n",
    "        print(f\"✅ Processed {ticker} successfully with {len(combined_df)} rows of combined data\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {ticker}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b03698",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedDataDict['CVS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079632c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.get_tickers_from_combined_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19863929",
   "metadata": {},
   "source": [
    "##### Example Backtest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8d53ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SIGNAL():\n",
    "    return df.signal\n",
    "\n",
    "class SignalStrategy(Strategy):\n",
    "    stopLoss = 1  # Represents percentage (e.g., 1 = 1%)\n",
    "    takeProfit = 15  # Represents percentage (e.g., 15 = 15%)\n",
    "    \n",
    "    def init(self):\n",
    "        self.signal_indicator = self.I(lambda: self.data.df['signal'])\n",
    "    \n",
    "    def next(self):\n",
    "        try:\n",
    "            current_signal = self.signal_indicator[-1]\n",
    "            current_price = self.data.Close[-1]\n",
    "            \n",
    "            # Calculate SL/TP as percentages of current price\n",
    "            sl_percent = self.stopLoss * 0.01\n",
    "            tp_percent = self.takeProfit * 0.01\n",
    "            \n",
    "            if current_signal == 0 and self.position:\n",
    "                self.position.close()\n",
    "                \n",
    "            elif current_signal == 1:  # LONG signal\n",
    "                if self.position.is_short:\n",
    "                    self.position.close()\n",
    "                    \n",
    "                if not self.position.is_long:\n",
    "                    # For LONG: SL below price, TP above price\n",
    "                    sl_price = current_price * (1 - sl_percent)\n",
    "                    tp_price = current_price * (1 + tp_percent)\n",
    "                    self.buy(sl=sl_price, tp=tp_price)\n",
    "                    \n",
    "            elif current_signal == -1:  # SHORT signal\n",
    "                if self.position.is_long:\n",
    "                    self.position.close()\n",
    "                    \n",
    "                if not self.position.is_short:\n",
    "                    # For SHORT: SL above price, TP below price\n",
    "                    sl_price = current_price * (1 + sl_percent)\n",
    "                    tp_price = current_price * (1 - tp_percent)\n",
    "                    self.sell(sl=sl_price, tp=tp_price)\n",
    "                    \n",
    "        except IndexError:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "# bt = Backtest(\n",
    "#     df,\n",
    "#     SignalStrategy,\n",
    "#     cash=10_000,\n",
    "#     commission=.002\n",
    "# )\n",
    "\n",
    "# stats, heatmap = bt.optimize(\n",
    "#     stopLoss = range(1, 5, 1),\n",
    "#     takeProfit = range(12, 20, 1),\n",
    "#     maximize='Sharpe Ratio',\n",
    "#     return_heatmap=True,\n",
    "#     max_tries= 50,\n",
    "# )\n",
    "# self.backTests[ticker] = bt\n",
    "# self.tickerResults[ticker] = stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ebfb9e",
   "metadata": {},
   "source": [
    "##### Trading Strategy Abstract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5f67cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TradingStrategy():\n",
    "    def __init__(self, data, window=30):\n",
    "        self.rawData = data\n",
    "        self.window = window\n",
    "        self.workingData = pd.DataFrame()\n",
    "        self.processed = False\n",
    "        self.tickerResults = {}\n",
    "        self.backTests = {}\n",
    "        \n",
    "    @abstractmethod\n",
    "    def createProcessedData(self):\n",
    "        \"\"\"Preprocess raw data into working format\"\"\"\n",
    "        pass\n",
    "        \n",
    "    @abstractmethod\n",
    "    def calculateMetrics(self):\n",
    "        \"\"\"Calculate strategy-specific metrics and signals\"\"\"\n",
    "        pass\n",
    "        \n",
    "    @abstractmethod\n",
    "    def addSignalsToDataFrames(self):\n",
    "        \"\"\"Add generated signals to individual asset DataFrames\"\"\"\n",
    "        pass\n",
    "        \n",
    "    @abstractmethod\n",
    "    def executeBacktests(self):\n",
    "        \"\"\"Execute backtests for all assets\"\"\"\n",
    "        pass\n",
    "        \n",
    "    def go(self):\n",
    "        \"\"\"Main execution workflow\"\"\"\n",
    "        self.createProcessedData()\n",
    "        self.calculateMetrics()\n",
    "        self.addSignalsToDataFrames()\n",
    "        self.executeBacktests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ca03a3",
   "metadata": {},
   "source": [
    "##### Value Strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e803258",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EbitdaValueStrategy(TradingStrategy):\n",
    "    def createProcessedData(self):\n",
    "        self.filterRawData()\n",
    "        \"\"\"Preprocess raw data into working format\"\"\"\n",
    "        column_map = {\n",
    "            'open': 'Open',\n",
    "            'high': 'High',\n",
    "            'low': 'Low',\n",
    "            'close': 'Close',\n",
    "            'volume': 'Volume'\n",
    "        }\n",
    "        \n",
    "        for ticker in self.rawData.keys():\n",
    "            df = self.rawData[ticker].copy()\n",
    "            # Drop the symbol column if it exists\n",
    "            if 'symbol' in df.columns:\n",
    "                df.drop(columns=['symbol'], inplace=True)\n",
    "            # Rename columns using the mapping\n",
    "            df.rename(columns=column_map, inplace=True)\n",
    "            # If High or Low columns don't exist, create them as a copy of Close\n",
    "            if 'High' not in df.columns:\n",
    "                df['High'] = df['Close']\n",
    "            if 'Low' not in df.columns:\n",
    "                df['Low'] = df['Close']\n",
    "            self.rawData[ticker] = df\n",
    "            \n",
    "            min_date = min(df.index.min() for df in self.rawData.values())\n",
    "            print(f\"Minimum date after processing: {min_date}\")\n",
    "            \n",
    "    def createValueData(self):\n",
    "        # 1. Create EBITDA DataFrame\n",
    "        value_df = pd.DataFrame()\n",
    "        rawData = self.rawData.copy()\n",
    "        hasEbitda = 0\n",
    "        # Collect EBITDA values for each ticker\n",
    "        for ticker, df in rawData.items():\n",
    "            if 'ebitdaValue' in df.columns:\n",
    "                hasEbitda += 1\n",
    "                # Ensure we have a datetime index\n",
    "                temp = df[['ebitdaValue']].copy()\n",
    "                temp.index = pd.to_datetime(temp.index)\n",
    "                \n",
    "                # Rename column to ticker name\n",
    "                temp = temp.rename(columns={'ebitdaValue': ticker})\n",
    "                \n",
    "                if value_df.empty:\n",
    "                    value_df = temp\n",
    "                else:\n",
    "                    value_df = value_df.join(temp, how='outer')\n",
    "                \n",
    "                \n",
    "        \n",
    "        #Make the Nan values 0\n",
    "        value_df.fillna(0, inplace=True)\n",
    "        \n",
    "        self.value_df = value_df\n",
    "    \n",
    "    def getTopBottomTickers(self):\n",
    "        value_df = self.value_df.copy()\n",
    "\n",
    "        # 3. Rank values row-wise (date-wise)\n",
    "        rank_df = value_df.rank(axis=1, method='min', ascending=False)\n",
    "        rank_df.dropna(axis=1, how='all', inplace=True)  # Drop columns with all NaN values\n",
    "\n",
    "        self.rank_df = rank_df\n",
    "        \n",
    "        # 4. Identify top/bottom tickers\n",
    "        top_bottom_tickers = set()\n",
    "        \n",
    "        # Iterate through each date\n",
    "        for date, row in rank_df.iterrows():\n",
    "            # Get top 50 and bottom 50 tickers for this date\n",
    "            sorted_tickers = row.sort_values().index\n",
    "            #removing the tickers that have 0 as the value\n",
    "            sorted_tickers = [ticker for ticker in sorted_tickers if row[ticker] > 0]\n",
    "            window = self.window\n",
    "            if len(sorted_tickers) < self.window * 2:\n",
    "                window = math.floor(self.window/2)\n",
    "                continue\n",
    "            top_tickers = sorted_tickers[:self.window]\n",
    "            bottom_tickers = sorted_tickers[-self.window:]\n",
    "            \n",
    "            # Add to our set\n",
    "            top_bottom_tickers.update(top_tickers)\n",
    "            top_bottom_tickers.update(bottom_tickers)\n",
    "            \n",
    "        return top_bottom_tickers\n",
    "            \n",
    "    def filterRawData(self):\n",
    "        # getting the lowest minimum date of all the dataframes in rawData\n",
    "        min_date = min(df.index.min() for df in self.rawData.values())\n",
    "        print(f\"Minimum date before filtration: {min_date}\")\n",
    "        \n",
    "        self.createValueData()\n",
    "        top_bottom_tickers = self.getTopBottomTickers()\n",
    "        \n",
    "        # 5. Filter rawData to keep only top/bottom tickers\n",
    "        filtered_rawData = {\n",
    "            ticker: df \n",
    "            for ticker, df in self.rawData.items() \n",
    "            if ticker in top_bottom_tickers\n",
    "        }\n",
    "        \n",
    "        self.rawData = filtered_rawData\n",
    "        \n",
    "        min_date = min(df.index.min() for df in self.rawData.values())\n",
    "        print(f\"Minimum date after filtration: {min_date}\")\n",
    "        \n",
    "        # Only keep columns in rank_df that are in filtered_rawData\n",
    "        filtered_columns = [col for col in self.rank_df.columns if col in filtered_rawData.keys()]\n",
    "        self.rank_df = self.rank_df[filtered_columns]\n",
    "        # Create the rankedDict {date: {ticker: rank}}\n",
    "        rankedDict = {}\n",
    "        for date, row in self.rank_df.iterrows():\n",
    "            date_ranks = {}\n",
    "            for ticker, rank_value in row.items():\n",
    "                date_ranks[ticker] = int(rank_value)  # Convert to integer rank\n",
    "            rankedDict[date] = date_ranks\n",
    "            \n",
    "        self.rankedDict = rankedDict\n",
    "        \n",
    "\n",
    "        print(f\"Original tickers: {len(self.rawData)}\")\n",
    "        print(f\"Filtered tickers: {len(filtered_rawData)}\")\n",
    "        \n",
    "    def mergedData(self):\n",
    "        min_date = min(df.index.min() for df in self.rawData.values())\n",
    "        print(f\"Minimum date before margedData: {min_date}\")\n",
    "        \n",
    "        for ticker in self.rawData.keys():\n",
    "            df = self.rawData[ticker].copy()\n",
    "            # Drop the symbol column if it exists\n",
    "            if 'symbol' in df.columns:\n",
    "                df.drop(columns=['symbol'], inplace=True)\n",
    "            # Always add prefix for consistency\n",
    "            df = df.add_prefix(f\"{ticker}_\")\n",
    "            if self.workingData.empty:\n",
    "                self.workingData = df.copy()\n",
    "            else:\n",
    "                self.workingData = self.workingData.join(df, how='outer')\n",
    "            # If {ticker}_symbol column exists, drop it\n",
    "            symbol_col = f\"{ticker}_symbol\"\n",
    "            if symbol_col in self.workingData.columns:\n",
    "                self.workingData.drop(columns=[symbol_col], inplace=True)\n",
    "        print(f\"Minimum date of workingData after mergedData: {self.workingData.index.min()}\")\n",
    "        min_date = min(df.index.min() for df in self.rawData.values())\n",
    "        print(f\"Minimum date after mergedData: {min_date}\")\n",
    "            \n",
    "        # Ensure the workingData DataFrame is sorted by date index\n",
    "        self.workingData.sort_index(inplace=True)\n",
    "        \n",
    "        #only keep the dates are are in self.dataDict.keys()\n",
    "        \n",
    "    \n",
    "    def getRanks(self, ticker):\n",
    "        \"\"\"Get ranks for a specific ticker based on the rankedDict\"\"\"\n",
    "        # Create a new Series with the same index as self.workingData\n",
    "        signal_series = pd.Series(0, index=self.workingData.index)\n",
    "        #print the mimimum date of the signal_series\n",
    "        print(f\"Minimum date of signal_series: {signal_series.index.min()}\")\n",
    "        \n",
    "        # Iterate through the date in the index of signal_series\n",
    "        for date in signal_series.index:\n",
    "            if date in self.rankedDict:\n",
    "                rank = self.rankedDict[date].get(ticker, 0)\n",
    "                if 1 <= rank <= self.window:\n",
    "                    signal_series[date] = 1\n",
    "                elif len(self.rankedDict[date]) - self.window < rank <= len(self.rankedDict[date]):\n",
    "                    signal_series[date] = -1\n",
    "                else:\n",
    "                    signal_series[date] = 0\n",
    "            else:\n",
    "                signal_series[date] = 0\n",
    "        return signal_series          \n",
    "    \n",
    "    def calculateMetrics(self):\n",
    "        \"\"\"Calculate strategy-specific metrics and signals\"\"\"\n",
    "        #the signal for each ticker will be in the column {ticker}_signal in the self.mergedData DataFrame\n",
    "        #for each ticker, if the value for that date in rankedData is between 1 and 10, then the signal is 1\n",
    "        #for each ticker if the value for that date in rankedData is between number of Columns in self.rankedDf and \n",
    "        #number of Columns in self.rankedDf - 10, then the signal is -1\n",
    "        min_date = min(df.index.min() for df in self.rawData.values())\n",
    "        print(f\"Minimum date before calculateMetrics: {min_date}\")\n",
    "        \n",
    "        self.mergedData()\n",
    "        print(f\"Minimum date of workingData before calculateMetrics: {self.workingData.index.min()}\")\n",
    "        for ticker in self.rawData.keys():\n",
    "            # Create a new column for the signal\n",
    "            self.workingData[f\"{ticker}_signal\"] = 0\n",
    "            self.workingData[f\"{ticker}_signal\"] = self.getRanks(ticker)\n",
    "            #shift the symbol signal by 1 to avoid lookahead bias\n",
    "            self.workingData[f\"{ticker}_signal\"] = self.workingData[f\"{ticker}_signal\"].shift(1)\n",
    "        \n",
    "        min_date = min(df.index.min() for df in self.rawData.values())\n",
    "        print(f\"Minimum date after filtration: {min_date}\")\n",
    "        #print the mimimum date of the workingData DataFrame\n",
    "        #calculate the minimum date of the workingData DataFrame\n",
    "        print(f\"Minimum date of workingData after calculateMetrics: {self.workingData.index.min()}\")\n",
    "            \n",
    "            \n",
    "            #for each row put in the ran\n",
    "    \n",
    "    def addSignalsToDataFrames(self):\n",
    "        \"\"\"Add generated signals to individual asset DataFrames\"\"\"\n",
    "        # First ensure all indexes are unique\n",
    "        self.workingData = self.workingData[~self.workingData.index.duplicated(keep='last')]\n",
    "        \n",
    "        for ticker, df in self.rawData.items():\n",
    "            # Ensure current DF has unique index\n",
    "            df_clean = df[~df.index.duplicated(keep='last')]\n",
    "            \n",
    "            # Get intersection of dates\n",
    "            common_dates = df_clean.index.intersection(self.workingData.index)\n",
    "            \n",
    "            # Get signals only for existing common dates\n",
    "            if not common_dates.empty and f'{ticker}_signal' in self.workingData.columns:\n",
    "                signal_series = self.workingData.loc[common_dates, f'{ticker}_signal']\n",
    "                \n",
    "                # Add to a copy to avoid SettingWithCopyWarning\n",
    "                df_clean = df_clean.copy()\n",
    "                df_clean['signal'] = signal_series\n",
    "                \n",
    "                # Drop NA signals and update rawData\n",
    "                df_clean.dropna(subset=['signal'], inplace=True)\n",
    "                self.rawData[ticker] = df_clean\n",
    "                print(f\"✅ Signals added to {ticker} DataFrame.\")\n",
    "            else:\n",
    "                print(f\"⚠️ No signals available for {ticker}\")\n",
    "                \n",
    "        # Verify dates after processing\n",
    "        min_date = min(df.index.min() for df in self.rawData.values() if not df.empty)\n",
    "        print(f\"Minimum date after adding signals: {min_date}\")\n",
    "        \n",
    "    def calculateRollingSharpe(self, returns, window=90, risk_free_rate=0.0):\n",
    "        excess_returns = returns - risk_free_rate\n",
    "        rolling_mean = excess_returns.rolling(window).mean()\n",
    "        rolling_std = excess_returns.rolling(window).std()\n",
    "        return (rolling_mean / rolling_std) * np.sqrt(252)  # \n",
    "        \n",
    "    def calculateBacktestMetrics(self):\n",
    "        all_returns = {}\n",
    "        \n",
    "        for ticker in self.tickerResults:\n",
    "            tickerEquityCurve = self.tickerResults[ticker]['_equity_curve'] ## is a dataframe\n",
    "            tickerEquityCurve['Return']= tickerEquityCurve['Equity'].pct_change().fillna(0)\n",
    "            combined_df['rolling_sharpe_90'] = self.calculateRollingSharpe(\n",
    "                                                tickerEquityCurve['Return'],\n",
    "                                                window=90,\n",
    "                                            )\n",
    "            \n",
    "            all_returns[ticker] = tickerEquityCurve['Return']\n",
    "            \n",
    "        returns_df = pd.DataFrame(all_returns)\n",
    "\n",
    "        # Step 3: Fill missing values with 0 (for dates before IPO)\n",
    "        returns_df.fillna(0, inplace=True)\n",
    "\n",
    "        # Step 4: Calculate strategy returns (sum across tickers)\n",
    "        strategy_daily_returns = returns_df.sum(axis=1)\n",
    "        \n",
    "        strategy_rolling_sharpe = self.calculateRollingSharpe(\n",
    "            strategy_daily_returns,\n",
    "            window=90,\n",
    "            risk_free_rate=0.0\n",
    "        )\n",
    "\n",
    "        # Step 5: Store results in the object\n",
    "        self.strategy_daily_returns = strategy_daily_returns\n",
    "        self.strategy_rolling_sharpe = strategy_rolling_sharpe\n",
    "        \n",
    "        self.calculatePerformanceMetrics()\n",
    "            \n",
    "    def calculatePerformanceMetrics(self):\n",
    "        \"\"\"Calculate performance metrics from strategy returns\"\"\"\n",
    "        returns = self.strategy_daily_returns\n",
    "        \n",
    "        # Annualized return\n",
    "        annualized_return = (1 + returns).prod() ** (252/len(returns)) - 1\n",
    "        \n",
    "        # Annualized volatility\n",
    "        annualized_vol = returns.std() * np.sqrt(252)\n",
    "        \n",
    "        # Sharpe ratio (assuming 0% risk-free rate)\n",
    "        sharpe_ratio = annualized_return / annualized_vol\n",
    "        \n",
    "        # Max drawdown\n",
    "        equity = (1 + returns).cumprod()\n",
    "        peak = equity.expanding(min_periods=1).max()\n",
    "        drawdown = (equity/peak) - 1\n",
    "        max_drawdown = drawdown.min()\n",
    "        \n",
    "        # Store metrics\n",
    "        self.performance_metrics = {\n",
    "            'Annualized Return': annualized_return,\n",
    "            'Annualized Volatility': annualized_vol,\n",
    "            'Sharpe Ratio': sharpe_ratio,\n",
    "            'Max Drawdown': max_drawdown,\n",
    "            'Total Return': equity.iloc[-1] / equity.iloc[0] - 1\n",
    "        }  \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    def executeBacktests(self):\n",
    "        min_date = min(df.index.min() for df in self.rawData.values())\n",
    "        print(f\"Minimum date before backtest: {min_date}\")\n",
    "        \"\"\"Execute backtests for all assets\"\"\"\n",
    "        startTime = time.perf_counter()\n",
    "        for ticker, df in self.rawData.items():\n",
    "            if 'signal' in df.columns:\n",
    "                #minimum date of the df DataFrame\n",
    "                if df.empty or df['signal'].isnull().all():\n",
    "                    print(f\"❌ No valid data for backtest on {ticker}. Skipping...\")\n",
    "                    continue\n",
    "                print(f\"Mimimum date of {ticker} DataFrame: {df.index.min()}\")\n",
    "                bt = Backtest(df, SignalStrategy, cash=10_000, commission=.002)\n",
    "                stats, heatmap = bt.optimize(\n",
    "                    stopLoss = range(1, 5, 1),\n",
    "                    takeProfit = range(12, 25, 1),\n",
    "                    maximize='Sharpe Ratio',\n",
    "                    return_heatmap=True,\n",
    "                    max_tries= 50,\n",
    "                )\n",
    "                self.backTests[ticker] = bt\n",
    "                self.tickerResults[ticker] = stats\n",
    "                print(f\"✅ Backtest executed for {ticker}.\")\n",
    "        endTime = time.perf_counter()\n",
    "        self.BackTestTime = endTime - startTime\n",
    "        print(f\"Total backtest execution time: {self.BackTestTime:.2f} seconds\")\n",
    "        \n",
    "    def executeMultiThreadedBacktests(self):\n",
    "        min_date = min(df.index.min() for df in self.rawData.values())\n",
    "        print(f\"Minimum date before backtest: {min_date}\")\n",
    "        \n",
    "        # Start the stopwatch\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        # Create a wrapper function for thread execution\n",
    "        def process_ticker(ticker):\n",
    "            \"\"\"Process a single ticker for backtesting\"\"\"\n",
    "            df = self.rawData[ticker]\n",
    "            if 'signal' not in df.columns or df.empty or df['signal'].isnull().all():\n",
    "                print(f\"❌ No valid data for backtest on {ticker}. Skipping...\")\n",
    "                return ticker, None, None\n",
    "            \n",
    "            print(f\"Minimum date of {ticker} DataFrame: {df.index.min()}\")\n",
    "            \n",
    "            try:\n",
    "                bt = Backtest(df, SignalStrategy, cash=10_000, commission=.002)\n",
    "                stats, heatmap = bt.optimize(\n",
    "                    stopLoss=range(1, 5, 1),\n",
    "                    takeProfit=range(12, 15, 1),\n",
    "                    maximize='Sharpe Ratio',\n",
    "                    return_heatmap=True,\n",
    "                    max_tries=50,\n",
    "                )\n",
    "                print(f\"✅ Backtest executed for {ticker}.\")\n",
    "                return ticker, bt, stats\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error during backtest for {ticker}: {str(e)}\")\n",
    "                return ticker, None, None\n",
    "        \n",
    "        # Create thread pool - adjust max_workers based on your CPU cores\n",
    "        max_workers = min(6, len(self.rawData))  # Don't exceed 8 threads\n",
    "        print(f\"Starting backtests with {max_workers} threads...\")\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit all tasks to the thread pool\n",
    "            future_to_ticker = {\n",
    "                executor.submit(process_ticker, ticker): ticker \n",
    "                for ticker in self.rawData.keys()\n",
    "            }\n",
    "            \n",
    "            # Process completed tasks as they finish\n",
    "            for future in concurrent.futures.as_completed(future_to_ticker):\n",
    "                ticker = future_to_ticker[future]\n",
    "                try:\n",
    "                    # Get the result from the thread\n",
    "                    _, bt, stats = future.result()  # Unpack but ignore the returned ticker\n",
    "                    \n",
    "                    # Check explicitly for None instead of truthiness\n",
    "                    if bt is not None and stats is not None:\n",
    "                        self.backTests[ticker] = bt\n",
    "                        self.tickerResults[ticker] = stats\n",
    "                    else:\n",
    "                        print(f\"⚠️ Backtest for {ticker} returned no results. Skipping...\")\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Unexpected error processing {ticker}: {str(e)}\")\n",
    "        \n",
    "        # Stop the stopwatch and calculate duration\n",
    "        end_time = time.perf_counter()\n",
    "        self.BacktestTime = end_time - start_time\n",
    "        \n",
    "        # Convert to minutes and seconds for readability\n",
    "        minutes = int(self.BacktestTime // 60)\n",
    "        seconds = self.BacktestTime % 60\n",
    "        \n",
    "        print(f\"\\n⏱️ Total backtest time: {minutes} minutes {seconds:.2f} seconds\")\n",
    "        print(f\"Number of tickers processed: {len(self.backTests)}\")\n",
    "        print(f\"Tickers with errors: {len(self.rawData) - len(self.backTests)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343a8adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "valStr = EbitdaValueStrategy(combinedDataDict)\n",
    "valStr.go()\n",
    "rawData = valStr.rawData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d8e3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "valStr.calculateBacktestMetrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36952b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "print(os.getenv('api_key'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48713c56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
